{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b37fdb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy.stats as stats\n",
    "from Robustness import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee546af",
   "metadata": {},
   "source": [
    "### Global and local graph measure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3672f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BA_Graph(n,m,seed):\n",
    "    '''\n",
    "    Inputs: n : number of nodes (int), m: new nodes are attaching with degree m (int), seed: indicator (int);\n",
    "    Output: Barabasi-Albert graph\n",
    "    '''\n",
    "    G = nx.barabasi_albert_graph(n,m,seed)\n",
    "    return G\n",
    "\n",
    "def ER_Graph(n,p,seed):\n",
    "    '''\n",
    "    Inputs: n : number of nodes (int), p: probability of connection between two nodes (0<=p<=1), seed: indicator (int);\n",
    "    Output: Erdos_Renyi graph\n",
    "    '''\n",
    "    G = nx.erdos_renyi_graph(n,p,seed)\n",
    "    return G\n",
    "\n",
    "def WS_Graph(n,k,p,seed):\n",
    "    '''\n",
    "    Inputs: n : number of nodes (int), K = degree of each node (int), p: probability of rewiring of an edge (0<=p<=1), seed: indicator (int);\n",
    "    Output: Watts_Strogatz graph\n",
    "    '''\n",
    "    G = nx.watts_strogatz_graph(n,k,p,seed)\n",
    "    return G\n",
    "\n",
    "#Function to find the distance between two nodes\n",
    "def Distance(x1, y1, x2, y2):\n",
    "    '''\n",
    "    Inputs: Cartesian coordinates of two nodes (x1,y1) and (x2,y2);\n",
    "    Outputs: Cartesian Distance between the two nodes \n",
    "    '''\n",
    "    return math.sqrt(math.pow((float(x1)-float(x2)), 2) + math.pow((float(y2)-float(y1)), 2))\n",
    "\n",
    "def HyperbolicGraphGenerator(n,k,seed):\n",
    "    '''\n",
    "    Inputs: n : number of nodes (int), k = targeted average degree, seed: indicator (int);\n",
    "    Output: Creates edgelist and nodelist of Hyperbolic Graph Generator model\n",
    "    '''\n",
    "    pathh = f'../Data/Model_Networks/HGG/{n}_{k}/'\n",
    "    NF = open(pathh + f'HGG_node_seed_{seed}.txt','w')\n",
    "    EF = open(pathh + f'HGG_edge_seed_{seed}.txt','w')\n",
    "    #dictionary for Cartesian coordonate of the node\n",
    "    CCN = {}\n",
    "    #dictionary for Cartesian distance of the edge\n",
    "    ECD = {}\n",
    "    for file in open(pathh + f'HGG_seed_{seed}.hg', 'r'):\n",
    "        l = file.strip().split('\\t')\n",
    "        if len(l) == 3:\n",
    "            NF.write('%s'%file)\n",
    "            CCN[l[0]]=l[1:]\n",
    "        \n",
    "        elif len(l) == 2:\n",
    "            source = l[0]\n",
    "            target = l[1]\n",
    "            ECD[file.strip()] = Distance(CCN[source][0], CCN[source][1], CCN[target][0], CCN[target][1])\n",
    "    for k in ECD.keys():\n",
    "        EF.write(\"%s\\t%f\\n\"%(k, ECD[k]))\n",
    "    NF.close()\n",
    "    EF.close()\n",
    "    print('node and edgelist created')\n",
    "    \n",
    "\n",
    "def measure(G,seed,path,model):\n",
    "    '''\n",
    "    Inputs: G: networkx graph, n: Number of nodes, \n",
    "    m: Number of edges to attach from a new node to existing nodes (for BA model) (1 <= m < n),\n",
    "    k: Each node is joined with its k nearest neighbors (for WS model),\n",
    "    p: Probability of rewiring each edge (for WS model) or Probability for edge creation (for ER model),\n",
    "    seed: indicator (int), \n",
    "    path: directory of the output files,\n",
    "    model: Name of the model network \n",
    "    Outputs: Creates text files of Degree, ClusteringCoefficient, BetweennessCentrality, EigenVectorCentrality, \n",
    "    ClosenessCentrality and global measures for each graph G with the given parameters values\n",
    "    '''\n",
    "    \n",
    "    ## Node measures\n",
    "    \n",
    "    Deg = nx.degree(G)\n",
    "    dfD = pd.DataFrame(Deg)\n",
    "    dfD.to_csv(path + f'{model}_Degree_seed_{seed}.txt', sep = '\\t', index = None,header = None)\n",
    "\n",
    "    CC = nx.clustering(G)\n",
    "    data_list = list(CC.items())\n",
    "    dfC = pd.DataFrame(data_list)\n",
    "    dfC.to_csv(path + f'{model}_ClusteringCoefficient_seed_{seed}.txt', sep = '\\t', index = None,header = None)\n",
    "    \n",
    "    BC = nx.betweenness_centrality(G, seed = 1)\n",
    "    data_list1 = list(BC.items())\n",
    "    dfB = pd.DataFrame(data_list1)\n",
    "    dfB.to_csv(path + f'{model}_BetweennessCentrality_seed_{seed}.txt', sep = '\\t', index = None,header = None)\n",
    "    \n",
    "    EVC = nx.eigenvector_centrality(G, max_iter=5000)\n",
    "    data_list2 = list(EVC.items())\n",
    "    dfE = pd.DataFrame(data_list2)\n",
    "    dfE.to_csv(path + f'{model}_EigenVectorCentrality_seed_{seed}.txt', sep = '\\t', index = None,header = None)\n",
    "    \n",
    "    CloseC = nx.closeness_centrality(G)\n",
    "    data_list3 = list(CloseC.items())\n",
    "    dfCloseC = pd.DataFrame(data_list3)\n",
    "    dfCloseC.to_csv(path + f'{model}_ClosenessCentrality_seed_{seed}.txt', sep = '\\t', index = None,header = None)\n",
    "    \n",
    "    print('Node measures are done!')\n",
    "    \n",
    "    ## Global Measure\n",
    "    file = open(path + f'{model}_Global_seed_{seed}.txt','w')\n",
    "    \n",
    "    file.write('Number_of_nodes\\tNumber_of_edges\\tFraction_of_nodes_inlcc\\tAverage_degree\\tEdge_density\\tMean_shortest_path_length')\n",
    "    file.write('\\n')\n",
    "    NN = G.number_of_nodes()\n",
    "    EE = G.number_of_edges()\n",
    "\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    lcc = list(map(int,largest_cc)) \n",
    "    G_lcc = G.subgraph(largest_cc)\n",
    "    fr_lcc = len(lcc)/NN\n",
    "    ave_deg = np.mean(dfD[1])\n",
    "    edge_density = 2*EE/(NN*(NN-1))\n",
    "    mean_spl = nx.average_shortest_path_length(G_lcc)\n",
    "    file.write(f'{NN}\\t{EE}\\t{fr_lcc}\\t{ave_deg}\\t{edge_density}\\t{mean_spl}')\n",
    "    file.close()\n",
    "    print('Global measures are done!')\n",
    "    \n",
    "    \n",
    "## To creat edgelist, nodelist and graph measures for model networks\n",
    "\n",
    "def input_files(model,n,m,k,k1,p,seed):\n",
    "    '''\n",
    "    Inputs: model: name of the model network,\n",
    "    n: Number of nodes, \n",
    "    m: Number of edges to attach from a new node to existing nodes (for BA model) (1 <= m < n),\n",
    "    k: Each node is joined with its k nearest neighbors (for WS model) ,\n",
    "    p: Probability of rewiring each edge (for WS model) or Probability for edge creation (for ER model),\n",
    "    seed: indicator (int),\n",
    "    Outputs: Save the nodelist, edgelist and other graph measures for the model network\n",
    "    '''\n",
    "    path = f'../Data/Model_Networks/{model}/'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for i in range(1,seed+1):\n",
    "        if model == 'HGG':\n",
    "            path1 = f'{n}_{k1}'\n",
    "            hgg = HyperbolicGraphGenerator(n,k1,i)\n",
    "            fe = pd.read_csv(path + path1 + f'/HGG_edge_seed_{i}.txt', sep = '\\t',header = None)\n",
    "            fn = pd.read_csv(path + path1 + f'/HGG_node_seed_{i}.txt', sep = '\\t',header = None)\n",
    "            G = nx.from_pandas_edgelist(fe,0,1)\n",
    "            G.add_nodes_from(fn[0])\n",
    "            \n",
    "        else:\n",
    "            if model == 'BA':\n",
    "                G = BA_Graph(n,m,i)\n",
    "                path1 = f'{n}_{m}'\n",
    "                if not os.path.exists(path + path1):\n",
    "                    os.mkdir(path + path1)\n",
    "\n",
    "            if model == 'ER':\n",
    "                G = ER_Graph(n,p,i)\n",
    "                path1 = f'{n}_{p}'\n",
    "                if not os.path.exists(path + path1):\n",
    "                    os.mkdir(path + path1)\n",
    "\n",
    "            if model == 'WS':\n",
    "                G = WS_Graph(n,k,p,i)\n",
    "                path1 = f'{n}_{k}_{p}'\n",
    "                if not os.path.exists(path + path1):\n",
    "                    os.mkdir(path + path1)\n",
    "\n",
    "            ## To write edgelist\n",
    "            nx.write_edgelist(G,path + f'{path1}/{model}_edge_seed_{i}.txt', data=False, delimiter='\\t')\n",
    "\n",
    "            ## To write nodelist\n",
    "            N = pd.DataFrame(G.nodes())\n",
    "            N.to_csv(path + f'{path1}/{model}_node_seed_{i}.txt', sep = '\\t', header = None, index = None)\n",
    "            print('node and edgelist created')\n",
    "        ## Calls the measure function for graph measures\n",
    "        infile = measure(G,i,path+path1+'/',model)\n",
    "        print(f'Done for seed {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9041fdd",
   "metadata": {},
   "source": [
    "## Generate Model Network and store it's Graph measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f09ba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node and edgelist created\n",
      "Node measures are done!\n",
      "Global measures are done!\n",
      "Done for seed 1\n",
      "Done for 1000.\n",
      "------------------------------- Done for BA.-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Generates all the model networks\n",
    "# Models = ['BA','ER','WS','HGG']\n",
    "# no_of_nodes = [1000,2000,5000]\n",
    "# M = [2,3,4,5]\n",
    "# K = [2,4,6,8,10]\n",
    "# KHGG = [3,5,7,9,10]\n",
    "# prob = [0.003,0.005,0.007,0.01]\n",
    "# seed = 100\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "no_of_nodes = [1000]\n",
    "M = [3]\n",
    "K = [6]\n",
    "KHGG = [3]\n",
    "prob = [0.005]\n",
    "seed = 1\n",
    "model = 'BA'\n",
    "\n",
    "for n in no_of_nodes:\n",
    "    if model == 'BA':\n",
    "        k,k1,p = 0,0,0\n",
    "        for m in M:\n",
    "            a = input_files(model,n,m,k,k1,p,seed)\n",
    "    elif model == 'ER':\n",
    "        k,k1,m = 0,0,0\n",
    "        for p in prob:\n",
    "            a = input_files(model,n,m,k,k1,p,seed)\n",
    "    elif model == 'WS':\n",
    "        m,k1,p = 0,0,0.5\n",
    "        for k in K:\n",
    "            a = input_files(model,n,m,k,k1,p,seed)\n",
    "    elif model == 'HGG':\n",
    "        m,k,p = 0,0,0\n",
    "        for k1 in KHGG:\n",
    "            a = input_files(model,n,m,k,k1,p,seed)\n",
    "    print(f'Done for {n}.')  \n",
    "print(f'------------------------------- Done for {model}.-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24b3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Node_minimum(nodefile,edgefile):\n",
    "    '''\n",
    "    Inputs: nodefile: dataframe of nodefile, edgefile: dataframe of edgefile, \n",
    "    Outputs: dataframe with four columns; c1:node name, c2: Minimum curvature value, c3: Sum of the curvature values,\n",
    "    c4: Average of the curvature values\n",
    "    '''\n",
    "    nodes = list(nodefile)\n",
    "    LISTS = {'nodes':[], 'minimum':[], 'sum':[], 'average':[]}\n",
    "    for node in nodes:\n",
    "        condition = ( edgefile[0] == node)\n",
    "        result_list = edgefile.loc[condition, 2].tolist()\n",
    "        LISTS['nodes'].append(node)\n",
    "        LISTS['minimum'].append(min(result_list))\n",
    "        LISTS['sum'].append(sum(result_list))\n",
    "        LISTS['average'].append(np.mean(result_list))\n",
    "    df = pd.DataFrame(LISTS)\n",
    "    return df\n",
    "\n",
    "def LCC(edgelist):\n",
    "    '''\n",
    "    Input: Pandas edgelist \n",
    "    Output: List of nodes present in the Largest connected component\n",
    "    '''\n",
    "    G = nx.from_pandas_edgelist(edgelist,0,1)\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    lcc = list(map(int,largest_cc))\n",
    "    return lcc\n",
    "\n",
    "def nodes_measure_lcc(seed,model,parameter,measure,lcc):\n",
    "    '''\n",
    "    Inputs: seed: integer,\n",
    "    model: Model network,\n",
    "    path: parameter values of the model network (directory),\n",
    "    measure: Name of the graph measure,\n",
    "    lcc: List of nodes present in the Largest connected component,\n",
    "    Output: Pandas dataframe with nodes name in first colume and corresponding measure's value in the second column\n",
    "    '''\n",
    "    path1 = '../Data/Model_Networks/'\n",
    "    \n",
    "    if measure in Graph_measure:\n",
    "        df1 = pd.read_csv(path1+ f'{model}/{parameter}/{model}_{measure}_seed_{seed}.txt', sep = '\\t', header = None)\n",
    "        df2 = df1[df1[0].isin(lcc)]\n",
    "        return df2\n",
    "  \n",
    "    elif measure in ['BakryEmery', 'Ollivier']:\n",
    "        df1 = pd.read_csv(path1+ f'{model}/{parameter}/{measure}/{model}_node_seed_{seed}.txt', sep = '\\t', header = None)\n",
    "        df1[0] = df1[0].astype(int)\n",
    "        df2 = df1[df1[0].isin(lcc)]\n",
    "        return df2\n",
    "    elif measure in ['Forman','AugForman']:\n",
    "        df1 = pd.read_csv(path1+ f'{model}/{parameter}/{measure}/{model}_node_seed_{seed}.txt', sep = '\\t', header = None)\n",
    "        dfe = pd.read_csv(path1 + f'{model}/{parameter}/{measure}/{model}_edge_seed_{seed}.txt',sep = '\\t', header = None)\n",
    "        df1[0] = df1[0].astype(int)\n",
    "        dfn = df1[df1[0].isin(lcc)]\n",
    "        df2 = Node_minimum(dfn[0],dfe)\n",
    "        return df2\n",
    "    else:\n",
    "        return \"Measure should be from ['BakryEmery','Forman','AugForman','Ollivier', 'Degree', 'ClusteringCoefficient','BetweennessCentrality','EigenVectorCentrality','ClosenessCentrality']\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639ba207",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models = ['BA','ER','WS','HGG']\n",
    "\n",
    "Graph_measure = ['Degree', 'ClusteringCoefficient','BetweennessCentrality','EigenVectorCentrality','ClosenessCentrality']\n",
    "Curv_measure = ['BakryEmery','Forman','AugForman','Ollivier']\n",
    "all_measures = Curv_measure + Graph_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b598c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure Done for 1000_3\n",
      "------------------- Measure Done for BA 1000_3 model----------------------------\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------\n",
    "## Creates text files of nodes measure present in lcc\n",
    "#----------------------------------------------------------\n",
    "\n",
    "seed = 1\n",
    "path1 = '../Data/Model_Networks/'\n",
    "model = 'BA'\n",
    "p = '1000_3'\n",
    "for i in range(1,seed+1):\n",
    "    df = pd.read_csv(path1 + f'{model}/{p}/{model}_edge_seed_{i}.txt', sep = '\\t', header = None)\n",
    "    lcc = LCC(df)\n",
    "    for measure in all_measures:\n",
    "        # Call the function nodes_measure_lcc to consider the node present in the Largest connected components\n",
    "        node_lcc = nodes_measure_lcc(i,model,p,measure,lcc)\n",
    "        if measure in Graph_measure:\n",
    "            node_lcc.to_csv(path1+ f'{model}/{p}/{model}_{measure}lcc_seed_{i}.txt', sep = '\\t', header = None, index = None)\n",
    "        elif measure == 'BakryEmery':    \n",
    "            node_lcc.to_csv(path1+ f'{model}/{p}/{measure}/{model}_nodelcc_seed_{i}.txt',sep = '\\t', header = None, index = None)\n",
    "        else:    \n",
    "            node_lcc.to_csv(path1+ f'{model}/{p}/{measure}/{model}_nodelcc_min_seed_{i}.txt',sep = '\\t', header = None, index = None)\n",
    "print(f'Measure Done for {p}') \n",
    "print(f'------------------- Measure Done for {model} {p} model----------------------------')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86a8cb",
   "metadata": {},
   "source": [
    "### Calculates Correlation between two vertex measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ff618a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------\n",
    "## Calculates Spearman and Preason Correlation between two measure\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def correlation(model,measure1,measure2,seed,path):\n",
    "    '''\n",
    "    Inputs: model: model's name, measure1: first vertex measure , measure2: second vertex measure, seed: total number to sample, path: directory;\n",
    "    Outputs: dictionary with Spearman and Preason correlation between measure1 and measure2 corresponding to each seed and creates text files of correlations \n",
    "    '''\n",
    "    corr = {'seed':[],'Spearman':[],'Pearson':[]}    \n",
    "    for i in range(1,seed+1):\n",
    "        if measure1 in Graph_measure:\n",
    "            df1 = pd.read_csv(path + f'{model}_{measure1}lcc_seed_{i}.txt', sep = '\\t', header = None)\n",
    "            v1 = list(df1[1])\n",
    "           \n",
    "        else:    \n",
    "            if measure1 == 'BakryEmery':\n",
    "                df1 = pd.read_csv(path + f'{measure1}/{model}_nodelcc_seed_{i}.txt',sep = '\\t', header = None)\n",
    "                v1 = list(df1[1])\n",
    "            else:\n",
    "                df1 = pd.read_csv(path + f'{measure1}/{model}_nodelcc_min_seed_{i}.txt',sep = '\\t', header = None)\n",
    "                v1 = list(df1[1])\n",
    "                \n",
    "        k1 = list(df1[0])\n",
    "        data_dict1 = {k1: v1 for k1, v1 in zip(k1, v1)}\n",
    "            \n",
    "        if measure2 in Graph_measure:\n",
    "            df2 = pd.read_csv(path + f'{model}_{measure2}lcc_seed_{i}.txt', sep = '\\t',header=None)\n",
    "            v2 = list(df2[1])\n",
    "        else:    \n",
    "            if measure2 == 'BakryEmery':\n",
    "                df2 = pd.read_csv(path + f'{measure2}/{model}_nodelcc_seed_{i}.txt',sep = '\\t', header = None)\n",
    "                v2 = list(df2[1])\n",
    "            else:\n",
    "                df2 = pd.read_csv(path + f'{measure2}/{model}_nodelcc_min_seed_{i}.txt',sep = '\\t', header=None)\n",
    "                v2 = list(df2[1])\n",
    "                \n",
    "        k2 = list(df2[0])\n",
    "        if len(k1) != len(k2):\n",
    "            return measure1,measure2,'\\nLength of the two input lists are not same',  str(len(k1)), str(len(k2))\n",
    "        \n",
    "        data_dict2 = {k: v for k, v in zip(k2, v2)}\n",
    "        set1 = list(data_dict1.values())\n",
    "        set2 = [data_dict2[key] for key in data_dict1.keys()]   \n",
    "        spear, p = stats.spearmanr(set1, set2)\n",
    "        pear, p = stats.pearsonr(set1, set2)\n",
    "        \n",
    "        corr['seed'].append(i)\n",
    "        corr['Spearman'].append(spear)\n",
    "        corr['Pearson'].append(pear)\n",
    "    df = pd.DataFrame(corr)   \n",
    "    if not os.path.exists(path + '/Correlation_Example'):\n",
    "        os.mkdir(path + '/Correlation_Example')\n",
    "    df.to_csv(path + f'/Correlation_Example/{model}_corr_{measure1}&{measure2}.txt',sep = '\\t', index = None)\n",
    "    #print(f'Done for {measure1} & {measure2}.')\n",
    "    return corr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c44cb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for BakryEmery & ClusteringCoefficient.\n",
      "Done for BakryEmery & Forman.\n",
      "Done for BakryEmery & BetweennessCentrality.\n",
      "Done for BakryEmery & AugForman.\n",
      "Done for BakryEmery & ClosenessCentrality.\n",
      "Done for BakryEmery & Degree.\n",
      "Done for BakryEmery & EigenVectorCentrality.\n",
      "Done for BakryEmery & Ollivier.\n"
     ]
    }
   ],
   "source": [
    "model = 'BA'\n",
    "p = '1000_3'        \n",
    "m1 = 'BakryEmery'\n",
    "path1 = '../Data/Model_Networks/'\n",
    "\n",
    "## Calculates Correlation between Bakry-Emery curvature and other vertex measures\n",
    "for m2 in list(set(all_measures)-{'BakryEmery'}):\n",
    "    cor = correlation(model,m1,m2,seed,path1 +f'{model}/{p}/')\n",
    "    print(f'Done for {m1} & {m2}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe23c85",
   "metadata": {},
   "source": [
    "### Robustness of the vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_measure = ['Degree', 'ClusteringCoefficient','BetweennessCentrality','EigenVectorCentrality','ClosenessCentrality']\n",
    "Curv_measure = ['BakryEmery','Forman','AugForman','Ollivier']\n",
    "\n",
    "\n",
    "name = 'BakryEmery'\n",
    "path = f'../Data/Model_Networks/BA/1000_3/'\n",
    "if not os.path.exists(path + 'Robustness_Example'):\n",
    "    os.mkdir(path + 'Robustness_Example')\n",
    "    \n",
    "edgefile = pd.read_csv(path + 'BA_edge_seed_1.txt', sep = '\\t', header = None)\n",
    "edgefile = edgefile.astype(int)\n",
    "G = nx.from_pandas_edgelist(edgefile,0,1)\n",
    "lcc = LCC(edgefile)\n",
    "subgraph = G.subgraph(lcc)\n",
    "SGraph =  subgraph.copy()\n",
    "        #df = pd.read_csv(path + f'{measure}/{model}_nodelcc_seed_{seed}.txt',sep = '\\t', header = None)\n",
    "\n",
    "\n",
    "df = pd.read_csv(path + 'BakryEmery/BA_nodelcc_seed_1.txt',sep = '\\t', header = None)\n",
    "measure_value = {df[0][i]:df[1][i] for i in range(len(df))}\n",
    "sort_order = False\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "# Call the function 'Robustness_node' to calculate the robustnesss\n",
    "#\n",
    "# If name in Graph_measure: sort_order = True\n",
    "# If name in Curv_measure:  sort_order = False   \n",
    "#\n",
    "# For random vertex removal, call the function 'Robustness_random'\n",
    "#------------------------------------------------------------------------------------------\n",
    "Robust = Robustness_node(measure_value,SGraph,sort_order)\n",
    "    \n",
    "\n",
    "df2 = pd.DataFrame(Robust)\n",
    "Robustfile = df2.T\n",
    "Robustfile.columns = ['Fraction_of_nodes','Efficiency']\n",
    "Robustfile.to_csv(path + f'Robustness_Example/Robustness_BakryEmery.txt', sep = '\\t',index = None)\n",
    "print(' --------------------- Done for',name,'----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb80ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
