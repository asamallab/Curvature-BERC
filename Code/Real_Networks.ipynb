{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20942f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy.stats as stats\n",
    "from itertools import combinations\n",
    "from Robustness import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb42fa1f",
   "metadata": {},
   "source": [
    "### Global and local graph measure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b577d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "## Function for creating global and local graph measur\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "def Measure(file):\n",
    "    '''\n",
    "    Inputs: G: networkx graph, \n",
    "    file: Name of the real network \n",
    "    Outputs: Creates text files of Degree, ClusteringCoefficient, BetweennessCentrality, EigenVectorCentrality, \n",
    "    ClosenessCentrality and global measures for each graph G\n",
    "    '''\n",
    "    path = '../Data/Real_Networks/'\n",
    "    \n",
    "    f = pd.read_csv(path + f'{file}/{file}_Edge.txt', sep = '\\t',header = None)\n",
    "    G = nx.from_pandas_edgelist(f,0,1)\n",
    "    print(G)\n",
    "    N = pd.DataFrame(G.nodes())\n",
    "    N.to_csv(path + f'{file}/{file}_Node.txt', sep = '\\t', header = None, index = None)\n",
    "\n",
    "    Deg = nx.degree(G)\n",
    "    dfD = pd.DataFrame(Deg)\n",
    "    dfD.to_csv(path + f'{file}/{file}_Degree.txt', sep = '\\t', index = None,header = None)\n",
    "\n",
    "    CC = nx.clustering(G)\n",
    "    data_list = list(CC.items())\n",
    "    dfC = pd.DataFrame(data_list)\n",
    "    dfC.to_csv(path + f'{file}/{file}_ClusteringCoefficient.txt', sep = '\\t', index = None,header = None)\n",
    "    \n",
    "    BC = nx.betweenness_centrality(G, seed = 1)\n",
    "    data_list1 = list(BC.items())\n",
    "    dfB = pd.DataFrame(data_list1)\n",
    "    dfB.to_csv(path + f'{file}/{file}_BetweennessCentrality.txt', sep = '\\t', index = None,header = None)\n",
    "    \n",
    "    EVC = nx.eigenvector_centrality(G, max_iter=5000)\n",
    "    data_list2 = list(EVC.items())\n",
    "    dfE = pd.DataFrame(data_list2)\n",
    "    dfE.to_csv(path + f'{file}/{file}_EigenVectorCentrality.txt', sep = '\\t', index = None,header = None)\n",
    "    \n",
    "    CloseC = nx.closeness_centrality(G)\n",
    "    data_list3 = list(CloseC.items())\n",
    "    dfCloseC = pd.DataFrame(data_list3)\n",
    "    dfCloseC.to_csv(path + f'{file}/{file}_ClosenessCentrality.txt', sep = '\\t', index = None,header = None)\n",
    "\n",
    "    print(f'Node Measure done for {file}.')\n",
    "    \n",
    "    ## Global Measure\n",
    "    outfile = open(path + f'{file}/{file}_Global.txt','w')\n",
    "    outfile.write('Number_of_nodes\\tNumber_of_edges\\tFraction_of_nodes_inlcc\\tAverage_degree\\tEdge_density\\tMean_shortest_path_length')\n",
    "    outfile.write('\\n')\n",
    "    NN = G.number_of_nodes()\n",
    "    EE = G.number_of_edges()\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    lcc = list(map(int,largest_cc)) \n",
    "    G_lcc = G.subgraph(largest_cc)\n",
    "    fr_lcc = len(lcc)/NN\n",
    "    ave_deg = np.mean(dfD[1])\n",
    "    edge_density = 2*EE/(NN*(NN-1))\n",
    "    mean_spl = nx.average_shortest_path_length(G_lcc)\n",
    "    outfile.write(f'{NN}\\t{EE}\\t{fr_lcc}\\t{ave_deg}\\t{edge_density}\\t{mean_spl}')\n",
    "    outfile.close()\n",
    "    print(f'Global Measure done for {file}.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed8e87",
   "metadata": {},
   "source": [
    "### Functions for creating text files of nodes measure present in lcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad1e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------\n",
    "## Functions for creating text files of nodes measure present in lcc\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def Node_minimum(nodefile,edgefile):\n",
    "    '''\n",
    "    Inputs: nodefile: dataframe of nodefile, edgefile: dataframe of edgefile, \n",
    "    Outputs: dataframe with four columns; c1:node name, c2: Minimum curvature value, c3: Sum of the curvature values,\n",
    "    c4: Average of the curvature values\n",
    "    '''\n",
    "    nodes = list(nodefile)\n",
    "    LISTS = {'nodes':[], 'minimum':[], 'sum':[], 'average':[]}\n",
    "    for node in nodes:\n",
    "        condition = ( edgefile[0] == node)\n",
    "        result_list = edgefile.loc[condition, 2].tolist()\n",
    "        LISTS['nodes'].append(node)\n",
    "        LISTS['minimum'].append(min(result_list))\n",
    "        LISTS['sum'].append(sum(result_list))\n",
    "        LISTS['average'].append(np.mean(result_list))\n",
    "    df = pd.DataFrame(LISTS)\n",
    "    return df\n",
    "\n",
    "def LCC(edgelist):\n",
    "    '''\n",
    "    Input: Takes pandas edgelist \n",
    "    Output: List of nodes present in the Largest connected component\n",
    "    '''\n",
    "    G = nx.from_pandas_edgelist(edgelist,0,1)\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    lcc = list(map(int,largest_cc))\n",
    "    return lcc\n",
    "\n",
    "def nodes_measure_lcc(file,measure,lcc):\n",
    "    '''\n",
    "    Inputs: seed: file: real network's name,\n",
    "    measure: Name of the graph measure,\n",
    "    lcc: List of nodes present in the Largest connected component,\n",
    "    Output: Pandas dataframe with nodes name in first colume and corresponding measure's value in the second column\n",
    "    '''  \n",
    "    if measure in Graph_measure:\n",
    "        df1 = pd.read_csv(path + f'{file}/{file}_{measure}.txt', sep = '\\t', header = None)\n",
    "        df2 = df1[df1[0].isin(lcc)]\n",
    "        return df2    \n",
    "  \n",
    "    elif measure == 'BakryEmery':\n",
    "        df1 = pd.read_csv(path + f'{file}/{file}_{measure}_node.txt', sep = '\\t', header = None)\n",
    "        df1[0] = df1[0].astype(int)\n",
    "        df2 = df1[df1[0].isin(lcc)]\n",
    "        return df2\n",
    "    \n",
    "    else:\n",
    "        if measure == 'Ollivier':\n",
    "            df1 = pd.read_csv(path + f'{file}/{file}_{measure}_node_min.txt', sep = '\\t', header = None)\n",
    "            df1[0] = df1[0].astype(int)\n",
    "            df2 = df1[df1[0].isin(lcc)]\n",
    "            return df2\n",
    "        else:\n",
    "            df1 = pd.read_csv(path + f'{file}/{file}_{measure}_Node.txt', sep = '\\t', header = None)\n",
    "            dfe = pd.read_csv(path + f'{file}/{file}_{measure}_Edge.txt',sep = '\\t', header = None)\n",
    "            df1[0] = df1[0].astype(int)\n",
    "            df2 = Node_minimum(df1[0],dfe)\n",
    "            df2.to_csv(path + f'{file}/{file}_{measure}_node_min.txt', sep = '\\t', header = None, index = None)\n",
    "            df2 = df2[df2['nodes'].isin(lcc)]\n",
    "            return df2\n",
    "        \n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "## Calculates Spearman and Preason Correlation between two measure\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def correlation(file,measure1,measure2):\n",
    "    '''\n",
    "    Inputs: model: model's name, measure1: first vertex measure , measure2: second vertex measure, seed: total number to sample, path: directory;\n",
    "    Outputs: dictionary with Spearman and Preason correlation between measure1 and measure2 corresponding to each seed and creates text files of correlations \n",
    "    '''\n",
    "    path = '../Data/Real_Networks/'\n",
    "    df1 = pd.read_csv(path + f'{file}/{file}_{measure1}_nodelcc.txt', sep = '\\t', header = None)\n",
    "    k1 = list(df1[0])\n",
    "    v1 = list(df1[1])\n",
    "        \n",
    "    data_dict1 = {k: v for k, v in zip(k1, v1)}\n",
    "\n",
    "    df2 = pd.read_csv(path + f'{file}/{file}_{measure2}_nodelcc.txt', sep = '\\t', header = None)\n",
    "    k2 = list(df2[0])\n",
    "    v2 = list(df2[1])\n",
    "\n",
    "    if len(k1) != len(k2):\n",
    "        print(measure1,measure2,'\\nLength of the two input lists are not same')\n",
    "        return measure1,measure2,'\\nLength of the two input lists are not same'\n",
    "        \n",
    "    data_dict2 = {k: v for k, v in zip(k2, v2)} \n",
    "\n",
    "    set1 = list(data_dict1.values())\n",
    "    set2 = [data_dict2[i] for i in data_dict1.keys()]\n",
    "    df = pd.DataFrame()\n",
    "    df[1] = set1\n",
    "    df[2] = set2\n",
    "    if not os.path.exists(path + f'{file}/Correlation_Example'):\n",
    "        os.mkdir(path + f'{file}/Correlation_Example')\n",
    "    df.to_csv(path + f'{file}/Correlation_Example/{file}_corr_{measure1}&{measure2}_nodelcc.txt', sep = '\\t', header = None, index = None)\n",
    "    print(f'{measure1} & {measure2} Done!')\n",
    "    \n",
    "    spear, p = stats.spearmanr(set1, set2)\n",
    "    pear, p = stats.pearsonr(set1, set2)\n",
    "\n",
    "    return spear, pear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1309c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "Graph_measure = ['Degree', 'ClusteringCoefficient','BetweennessCentrality','EigenVectorCentrality','ClosenessCentrality']\n",
    "Curv_measure = ['BakryEmery','Forman','AugForman','Ollivier']\n",
    "all_measures = Curv_measure + Graph_measure\n",
    "\n",
    "path = '../Data/Real_Networks/'\n",
    "Networks = [file for file in os.listdir(path) if '.' not in file]\n",
    "print(len(Networks))\n",
    "\n",
    "Networks = ['ArenasEmail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0812cee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 1133 nodes and 5451 edges\n",
      "Node Measure done for ArenasEmail.\n",
      "Global Measure done for ArenasEmail.\n",
      "------------------ ArenasEmail Done!   Total time:  23.33251714706421 --------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "## Call the function \"measure\" to generate local and global graph measures\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "for file in Networks:\n",
    "    t1 = time.time()\n",
    "    infile = Measure(file)\n",
    "    print(f'------------------ {file} Done!   Total time: ',time.time()-t1, '--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bcdbd58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for BakryEmery\n",
      "Done for Forman\n",
      "Done for AugForman\n",
      "Done for Ollivier\n",
      "Done for Degree\n",
      "Done for ClusteringCoefficient\n",
      "Done for BetweennessCentrality\n",
      "Done for EigenVectorCentrality\n",
      "Done for ClosenessCentrality\n",
      "----------------------------- Done for ArenasEmail ------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------\n",
    "## Creates text files of nodes measure present in lcc\n",
    "#----------------------------------------------------------\n",
    "\n",
    "for file in Networks:\n",
    "    p1 = os.listdir(path+f'{file}/')\n",
    "    df = pd.read_csv(path + f'{file}/{file}_Edge.txt', sep = '\\t',header = None)\n",
    "    lcc = LCC(df)\n",
    "    for measure in all_measures:\n",
    "        lccmeasure = nodes_measure_lcc(file,measure,lcc)\n",
    "        lccmeasure.to_csv(path + f'{file}/{file}_{measure}_nodelcc.txt', sep = '\\t', header = None, index = None)\n",
    "        print(f'Done for {measure}')\n",
    "    print(f'----------------------------- Done for {file} ------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86a8cb",
   "metadata": {},
   "source": [
    "### Calculates Correlation between two vertex measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "707d9254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BakryEmery & AugForman Done!\n",
      "BakryEmery & ClosenessCentrality Done!\n",
      "BakryEmery & EigenVectorCentrality Done!\n",
      "BakryEmery & Ollivier Done!\n",
      "BakryEmery & Forman Done!\n",
      "BakryEmery & ClusteringCoefficient Done!\n",
      "BakryEmery & BetweennessCentrality Done!\n",
      "BakryEmery & Degree Done!\n",
      "----------------------------- Done for ArenasEmail ------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------\n",
    "## Creates text files of correlation between two vertex measures\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "m1 = 'BakryEmery'\n",
    "for file in Networks:\n",
    "    p1 = os.listdir(path+f'{file}/')\n",
    "    corr = {'name':[],'Spearman':[],'Pearson':[]} \n",
    "    #com = list(combinations(all_measures, 2))\n",
    "    #for (m1,m2) in com:        \n",
    "    for m2 in list(set(all_measures)-{'BakryEmery'}):\n",
    "        cor = correlation(file,m1,m2)\n",
    "        corr['name'].append(f'{m1} & {m2}')\n",
    "        corr['Spearman'].append(cor[0])\n",
    "        corr['Pearson'].append(cor[1])\n",
    "    df = pd.DataFrame(corr, index = None) \n",
    "    df.to_csv(path + f'{file}/Correlation_Example/{file}_correlation.txt',sep = '\\t', index = None )\n",
    "    print(f'----------------------------- Done for {file} ------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe23c85",
   "metadata": {},
   "source": [
    "### Robustness of the vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a862f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Graph_measure = ['Degree', 'ClusteringCoefficient','BetweennessCentrality','EigenVectorCentrality','ClosenessCentrality']\n",
    "Curv_measure = ['BakryEmery','Forman','AugForman','Ollivier']\n",
    "ALL_Measures = Curv_measure + Graph_measure + ['Random']\n",
    "\n",
    "name = 'BakryEmery'\n",
    "path = f'../Data/Real_Networks/ArenasEmail/'\n",
    "if not os.path.exists(path + 'Robustness_Example'):\n",
    "    os.mkdir(path + 'Robustness_Example')\n",
    "    \n",
    "nodefile = pd.read_csv(path + 'ArenasEmail_Node.txt', sep = '\\t', header = None)\n",
    "edgefile = pd.read_csv(path + 'ArenasEmail_Edge.txt', sep = '\\t', header = None)\n",
    "edgefile = edgefile.astype(int)\n",
    "G = nx.from_pandas_edgelist(edgefile,0,1)\n",
    "G.add_nodes_from(nodefile[0])\n",
    "\n",
    "\n",
    "df = pd.read_csv(path + 'ArenasEmail_BakryEmery_node.txt',sep = '\\t', header = None)\n",
    "measure_value = {df[0][i]:df[1][i] for i in range(len(df))}\n",
    "sort_order = False\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "# Call the function 'Robustness_node' to calculate the robustnesss\n",
    "#\n",
    "# If name in Graph_measure: sort_order = True\n",
    "# If name in Curv_measure:  sort_order = False   \n",
    "#\n",
    "# For random vertex removal, call the function 'Robustness_random'\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "Robust = Robustness_node(measure_value,G,sort_order)\n",
    "    \n",
    "df2 = pd.DataFrame(Robust)\n",
    "Robustfile = df2.T\n",
    "Robustfile.columns = ['Fraction_of_nodes','Efficiency']\n",
    "Robustfile.to_csv(path + 'Robustness_Example/Robustness_BakryEmery.txt', sep = '\\t',index = None)\n",
    "print(' --------------------- Done for',name,'----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f440ec62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
